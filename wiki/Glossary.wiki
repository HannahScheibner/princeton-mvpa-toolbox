= Princeton Multi-Voxel Pattern Analysis ' glossary =

See [Manual] (Data structures section) for more information on terms relating to the way the data is stored by the toolbox.


<wiki:toc>


== block ==
A group of contiguous [#TR TR] from the same [#condition condition] in a particular [#run run]. Usually comprises multiple behavioral [#trial trials].

== classification ==
In the machine learning sense, classification means taking a labelled training data set and showing the classifier algorithm examples of each condition over and over until it can successfully identify the training data. Then, the classifier's generalization performance is tested by asking it to guess the conditions of new, unseen data points.

See: [ManualClassification Classification] in the manual.


== condition ==
The groups that you're trying to teach your classifier to distinguish, e.g. different tasks being performed by the subject in the experiment, or different stimuli being viewed.

== cross-validation ==
When you use n-minus-one/leave-one-out cross-validation classification, you iterate over your data multiple times. Each [#iteration iteration] involves a fresh classifier [#condition trained] on a subset of the data, and tested on the withheld data.

See: [ManualClassification#N-minus-one_(leave-one-out)_cross-validation N-minus-one (leave-one-out) cross-validation]

== feature selection ==
Deciding which of your features (e.g. voxels) you want to include in your analysis.

== generalization ==
Testing the performance of a trained classifier on previously-unseen (test) data

== header ==
See: [ManualDataStructures#Book-keeping_and_the_headers Data structure ' Book-keeping and the headers]

== history ==
A free-text field in the [#header header] that gets automatically appended to, creating a sort of narrative of that object's role in the analysis.

See [ManualDataStructures#Book-keeping_and_the_headers Data structure ' Book-keeping and the headers]

== iteration ==
Running the classifier once, using a particular subset of the data for testing, and the remainder for training. For example, you have 10 runs, you'll have 10 iterations, each time withholding a different run as the testing data.

See: [#n_minus_one_cross_validation n minus one cross validation] 

== leave-one-out ==
We use 'leave-one-out' and 'n-minus-one' interchangeably to refer to the [#cross-validation cross-validation] procedure that leaves out a different subsection (e.g. [#run run]) of the data each [#iteration iteration].

== mask ==
A boolean 3D (or maybe 2D) single-TR volume indicating which voxels are to be included.

See [#mask Data structure ' masks].

== name ==
Every [#object object] in the [#subj _subj_] structure has a name. This is a very important field, since it is used whenever accessing that object. The user is advised to refrain from accessing objects directly (e.g. subj.patterns{1}).

See: [#The_innards_of Data structure ' innards of the _subj_ structure] and [#Accessing_the_subj Advanced ' accessing _subj_ directly]

== n minus one cross validation == 
We use 'leave-one-out' and 'n-minus-one' interchangeably to refer to the [#_cross-validation cross-validation] procedure that leaves out a different subsection (e.g. [#run run]) of the data each [#iteration iteration].

== object ==
An example of one of the [#Data_structure 4 main data types], e.g. a single cell in _subj_._patterns_ or _subj.masks_. Contains a _mat_ field with all the data, as well other required fields such as [#name name], group_name, derived_from, [#header header] etc.

See: [#The_innards_of The innards of the subj structure]

== one-of-n ==
In this toolbox, this tends to refer a regressors matrix, to the idea that only a single condition can be active at any timepoint. This makes sense for basic/standard classification ' each timepoint belongs to one or other of the conditions, but not more than one at once.

Convolving regressors with a hemodynamic response function will lead to continuous-valued regressors, which may overlap (i.e. more than one condition may be non-zero at a given timepoint), which may violate some functions' one-of-n requirements.

[http://code.google.com/p/princeton-mvpa-toolbox/source/browse/trunk/core/util/check_1ofn_regressors.m Check_1ofn_regressors.m] allows you to test whether a matrix is one-of-n.

== pattern ==
A (features x timepoints) matrix, usually of voxel activities, but could also be PCA components, wavelet coefficients, GLM beta weights or a statmap.

See: [#pattern Data structure ' patterns].

== peeking ==
When you use your testing data set to help with voxel selection. Basically, this is a kind of cheating, and spuriously/illegitimately improves your classification by some margin.

See: [Manual].

== performance ==
The performance metric measures the similarity between the output produced by a classifier to the output it's supposed to produce.

See [ManualClassification#Performance Performance] in the Classification section of the manual.

== Pre-Classification ==
By this, we mean the normalization and feature selection steps that go on before after the data structure has been created but before beginning classification, e.g. [http://code.google.com/p/princeton-mvpa-toolbox/source/browse/trunk/core/preproc/zscore_runs.m zscore_runs.m] and [http://code.google.com/p/princeton-mvpa-toolbox/source/browse/trunk/core/preproc/feature_select.m feature_select.m].

See [ManualPreClassification pre-classification].

== regressors ==
For our purposes, the term 'regressors' refers to a set of values for each TR that denote the extent to which each condition is active. Used by statistical tests, and also as the teacher signal for the classifiers.

See: [ManualDataStructures#Regressors Data structure ' regressors].

== results ==
This is where all the information about [#_condition classification] is stored.

See: [ManualClassification#The_results_structure Classification ' results structure].

== run ==
A single scanning session. There are usually a handful of runs in a given hour-long experiment.

== selector ==
A set of labels for each [#_TR TR], e.g. where all the [#_run runs] start and finish, or which TRs should be used for [#_condition training] and which for testing on this [#iteration iteration].

See: [ManualDataStructures#Selectors Data structure ' selectors].

== statmap ==
The result of some kind of statistical test, usually performed separately for each voxel. For instance, the ANOVA yields a statmap of p values, one for each voxel. Each p value denotes the probability that that voxel varies significantly between conditions.

Statmaps are stored as patterns, since the term 'mask' is usually used to refer to a boolean 3D volume.

A mask can be created from a statmap by choosing all the voxels that are above/below some threshold.

See [ManualDataStructures#Masks Data structure ' masks] and [ManualPreClassification#Statmaps Pre-classification ' Statmaps].

== subj ==
See: [ManualDataStructures#Selectors Data structure ' selectors].

== testing ==
Presented a [#training trained] classifier with patterns that it has never seen before, and testing its performance.

== TR ==
Stands for 'time to repetition'. Basically, the time taken for the scanner to acquire a single 3D brain volume. We often use it (somewhat imprecisely) to mean a single timepoint (usually of about 2s).

== training ==
Showing a classifier lots of examples of a person's brain in condition A, and telling it each time, 'This is an example of the brain in condition A'. We then show it lots of examples of the same brain in condition B, also telling it which condition these brain examples came from. This process repeats until the classifier has learned which are which.

In reality, the examples tend to be interleaved with each other and presented in a different order each time. Most classifier algorithms can also deal with more than just two categories.

== trial ==
A behavioural trial in the experiment, that probably spans multiple [#TR TR]s. Multiple trials make up a [#block block].

== voxel selection ==
Whenever you apply a _mask_ to a _pattern_, you are selecting voxels. This term tends to be used more often in the machine learning context of 'feature selection' ' choosing which of the features (voxels) contain signal for the classification problem you are attempting.

See: 'Pre-classification ' Anova' in the [Manual].