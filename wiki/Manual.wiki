= Princeton multi-voxel pattern analysis manual =

----

== Table of Contents: ==
<wiki:toc>

----

[ContactDetails Contact Details]

If you're new to this site, you will probably want to check
out [:Setup] for installation instructions, then [:Tutorials] to
get started, and finally [:Manual] for more detailed coverage.

[WhatIsTheToolbox General Toolbox Details]

=== Classification ===


==== Introduction ' training, testing and generalization ====

In the machine learning sense, classification means taking a labelled training data set and showing the classifier algorithm examples of each condition over and over until it can successfully identify the training data. Then, the classifier's generalization performance is tested by asking it to guess the conditions of new, unseen data points.

In terms of fMRI experiments, this amounts to a very simple kind of mind-reading ' being able to tell something about what the subject was thinking about at a given moment by looking at the activity in their brain, based on how their brain looked in the past when thinking about similar kinds of things. In practice, it only tends to work for making very crude guesses about the kind of task being performed, or stimuli being viewed.

The way we tend to test this is to train on most of the data, and then test our classifier's generalization performance on the remainder. By training, we mean showing it lots of examples of that person's brain when in condition A, and telling it each time, 'This is an example of the brain in condition A'. We then show it lots of examples of the same brain in condition B, also telling it which condition these brain examples came from.

For instance, we might train the classifier on examples from runs 1-9 of a 10-run experiment, and then see whether it guesses correctly when faced with data from run 10.


==== Performance ====

The performance metric measures the similarity between the output produced by a classifier to the output it's supposed to produce.

The simplest performance metric for classification is to ask whether the maximally-active category unit from the classifier corresponds to the maximally (or only) active condition in the regressors/targets. If so, that timepoint is 'correct', otherwise it's 'wrong'. This is all that the [http://www.csbmb.princeton.edu/mvpa/docs/m2html/perfmet_maxclass.html ''perfmet_maxclass.m''] performance metric function does.

Another performance metric that's often used in Neural Networks is the mean squared error.

There are many alternative and more sophisticated ways of evaluating performance.

See '[#_Creating_your_own_3 Creating your own performance metric function]' if you don't want to use ''perfmet_maxclass.m''.


==== Creating your own performance metric function ====

If you want to create your own performanc metric, start by looking at the default [http://www.csbmb.princeton.edu/mvpa/docs/m2html/perfmet_maxclass.html perfmet_maxclass.m], and the [http://www.csbmb.princeton.edu/mvpa/docs/m2html/perfmet_template.html perfmet_template.m] template.

Here are the requirementss:

 * Should take in a (nOutputUnits x nTimepoints) ''acts'' matrix of responses from the testing function
 * Should take in a (nOutputUnits x nTimepoints) ''targs'' matrix of desired responses (supervised labels)
 * ''Args'' can contain any further information your custom perfmet function requires
 * Should return a ''perfmet'' structure that has a ''perf'' scalar field containing the overall performance according to your performance metric. Any other information that might be useful later can be stored in the ''perfmet'' structure.


==== N-minus-one (leave-one-out) cross-validation ====

The method of training on most of the data and generalization-testing on the remainder is somewhat wasteful and potentially misleading. It could be that generalization performance for a particular run is very good or very bad. So you might want to try withholding a different run for testing, and training on the remainder, using a fresh classifier, to see whether the same kind of performance is obtained.

N-minus-one (leave-one-out) cross-validation is really just that idea taken to its extreme. If we have 10 runs, then we will run 10 separate classifiers, each one being generalization-tested on a different 1/10 of the data having been trained on the remaining 9/10. This way, every timepoint gets a turn at being part of the test set, and 9 turns at being part of the training set.

The toolbox is set up to make this kind of procedure very easy


==== Backpropagation ====

Backpropagation is an algorithm for training a neural network, that is, for adjusting the weights connecting units so that a desired output is produced for a given input. It's a powerful algorithm, and we have found that the conjugate gradient variant that is set to be the default classifier for the toolbox learns quickly and generalizes well.

In order to use the default classifier in the toolbox, you will need a copy of the Matlab [http://www.mathworks.com/products/neuralnet/ Neural Networks toolbox]. If you don't have one, you can use [http://www.csbmb.princeton.edu/mvpa/docs/m2html/class_bp_netlab.html ''class_bp_netlab.m''] (not yet ready) instead, which uses the open source [http://www.ncrg.aston.ac.uk/netlab/ Netlab toolbox] backpropagation function instead.

See the Matlab Neural Networks toolbox [http://www.mathworks.com/access/helpdesk/help/toolbox/nnet/ documentation] for more information, such as how to examine the classifier weights or the activations of the hidden layer.

Note: the default algorithm for [http://www.csbmb.princeton.edu/mvpa/docs/m2html/train_bp.html train_bp.m] is conjugate gradient ('traincgb'), mainly for historical reasons. However, the default for the Netlab net is scaled conjugate gradient (which is similar to 'trainscg'). It's all much of a muchness, though we're starting to think that 'trainscg' may be the way forward ' see the [#_My_classifier_sometimes Howto's] Classification section.


==== Included classifiers ====

Currently, [#_Backpropagation backpropagation] is the only classifier algorithm included with the toolbox. We intend to rapidly expand the list of included classifiers in future releases, but in the meantime, it's extremely easy to [#_Creating_your_own_1 add new classifiers yourself].


==== Creating your own training function ====

If you want to create your own classifier training function, start by looking at the default [http://www.csbmb.princeton.edu/mvpa/docs/m2html/train_bp.html train_bp.m], and the [http://www.csbmb.princeton.edu/mvpa/docs/m2html/train_template.html train_template.m] template.

This training function will also need a corresponding [#_Creating_your_own_2 testing function] that actually tests generalization to unseen data. It is worth noting that some classifiers don't need a training phase, in which case the training function should simply return an empty scratchpad.

Here are the requirements, based on ''train_bp.m'':

 * should take in ''trainpats'' (''nFeatures'' x ''nTrainingTimepoints'') training patterns
 * should take in ''traintargs'' (''nOutputUnits'' x ''nTrainingTimepoints'')
 * should take in a ''train_args'' structure containing fields specific to the classifier
 * should return a ''scratchpad'' structure that the corresponding [#_Creating_your_own_2 testing function] knows how to use to test the classifier

Since we couldn't think of a sufficiently broad term to encompass all possible classifier algorithms, we have adopted the term 'output units' to refer generically to the number of rows in the teacher signal (i.e. supervised labels) being fed to the classifier.


==== Creating your own testing function ====

If you want to create your own classifier testing function, start by looking at the default [http://www.csbmb.princeton.edu/mvpa/docs/m2html/test_bp.html test_bp.m], and the [http://www.csbmb.princeton.edu/mvpa/docs/m2html/test_template.html test_template.m] template.

Most classifiers need to be trained by a [#_Creating_your_own_5 training function] before being tested. The training function will create the scratchpad that contains whatever information is required at testing.

Here are the requirements, based on ''test_bp.m'':

 * should take in ''testpats'' (''nFeatures'' x ''nTestingTimepoints'')
 * should take in ''testtargs'' (''nOutputUnits'' x ''nTestingTimepoints'')
 * should take in a ''scratchpad'' which contains whatever fields are needed by the testing algorithm (e.g. a trained net) created by the corresponding [#_Creating_your_own_5 training function]
 * should return an ''acts'' matrix (''nOutputUnits ''x ''nTestingTimepoints''), the same size as the ''testtargs'', which contains the classifier's guesses in response to the ''testpats''. These ''acts'' get compared to the ''testtargs''.
 * should return the ''scratchpad'', in case the test function added to it


==== The results structure ====

The ''results'' structure stores everything you might need after a classification analysis.

It's divided up by iteration, one for each iteration of the n-minus-one (or whatever cross-validation method described by the selector group passed to ''cross_validation.m''). The following fields are contained in an ''iteration'' (not in this order):

 * ''scratchpad''

This stores information about a particular classifier. For instance, the network and weights get stored here when using the backpropagation classifier (in the ''net'') field.

 * ''acts''

This is a matrix (''nOut'' x ''nTestTimepoints in this iteration'') containing all of the outputs from the classifier for each of the conditions at every test timepoint in this iteration

Â· ''perfmet'' and ''perf''

This is a structure containing the calculations required to calculate the [#_Performance performance] of the classifier. There are multiple ways in this can be calculated, but all are required to contain a ''perf'' scalar. This ''perf'' scalar is duplicated in ''iterations(i).perf'' for convenience.

Multiple performance metrics can be used to calculate a performance value. If multiple performance metrics are applied to the same data, then the ''perfmet'' field is a cell array.

 * ''train_idx'', ''test_idx'', ''rest_idx'', ''unknown_idx''

These index vectors are derived from the cross-validation selector index that was used to decide which TRs would be used for training/testing in ''cross_validation.m''.

TRs in the cross-validation selector index marked with 1s are included in ''train_idx'', 2s in ''test_idx'', 0s become ''rest_idx'' and all other values go into the ''unknown_idx''. Only ''train_idx'' and ''test_idx'' play any role in classification at all ' ''rest_idx'' and ''unknown_idx'' are only stored for completeness.

 * ''created''

As in the ''subj'' objects, this stores the arguments and function name used to create this object.

 * ''header''

More book-keeping information. You can add to the free-text narrative ''history'' field with [http://www.csbmb.princeton.edu/mvpa/docs/m2html/add_results_history.html ''add_results_history.m''].

There are no accessor functions (like ''get_object.m'' or ''set_mat.m'') for the results structure ' just edit it directly if you need to.

Finally, it is worth noting that ''results.total_perf'' stores the mean of all the ''results.iterations(i).perfmet.perf'' values. If there are multiple perfmet objects, then ''results.total_perf'' will be a vector.


[[Include(TroubleshootingClassification)]]


==== Avoiding spurious classification ====

There are various ways in which one can fool oneself into thinking that above-chance classification means something.

For instance, if you're peeking (feeding your entire data set, including your test data, into your voxel selection method), then it's possible to classify complete nonsense better than chance (see [#_Peeking Peeking]).

One essential check is to create a set of scrambled regressors that have the same properties as your real regressors, i.e. same number of TRs in each condition, balanced across runs etc. If the classifier consistently trains and generalizes with above-chance performance on testing data when you know that there is no regularity to the regressors, then it's back to the debugging drawing board. See [http://www.csbmb.princeton.edu/mvpa/docs/m2html/scrambled_regressors.html ''scrambled_regressors.m''] for a way of easily shuffling the order of your timepoints within a run.

Note: there are various ways in which one could scramble/shuffle the regressors matrix. The provided method is the simplest one ' but for specific experiments or situations, other methods that preserve some of the properties of the data might be better.


==== Further information ====

For further information, see the [#_Classification_1 Classification Howto's and occasionally-asked questions].


=== Exporting ===

See [#_Importing importing].


==== To AFNI ====

See [http://www.csbmb.princeton.edu/mvpa/docs/m2html/write_to_afni.html ''write_to_afni.m'']. This will write a pattern or a mask to a BRIK file, using an existing sample BRIK (from the same data set) to create the necessary header information. Uses [http://afni.nimh.nih.gov/sscc/ziad Ziad Saad's] [http://afni.nimh.nih.gov/afni/matlab AFNI-Matlab] library. See the ''Howtos / ''[#_Exporting_1 ''Exporting''] section for more information.


==== To BrainVoyager ====

As [#_From_BrainVoyager above].


==== To SPM ====

As [#_From_SPM above].


=== Advanced ===


==== Conventions ====

By convention, the order of the arguments used in toolbox function is intended to be fairly stereotyped, to make it easier to remember. If the 'subj' structure is one of the arguments,


==== Managing memory ====

The toolbox has a packrat mentality ' it hoards every version of the data that it processes. For instance, it keeps separate ''patterns'' for before and after zscoring with ''zscore_runs.m''. This makes sense, because it makes it easy to take a step back in the analysis path and re-run things with slightly different parameters. However, it can eventually clog up your RAM, especially with large patterns.

For this reason, if you know that you plan to apply a mask to your data (e.g. to exclude voxels from outside the cranium), then use that mask as the argument to [http://www.csbmb.princeton.edu/mvpa/docs/m2html/load_afni_pattern.html ''load_afni_pattern.m''] when loading in the data in the first place. Unfortunately, the core ''BrikLoad.m'' function loads in the entire volume first and then applies the mask, but if you're loading in separate runs at a time, then this shouldn't be a huge problem.

Since the data from the scanner is often fairly noisy, it's rarely necessary to use a ''double'' type to store all the significant figures, which is the default used by Matlab. See [:MVPA manual:How do I use singles rather than doubles] for further information about this.

Another useful tactic is to store large patterns that you're not using at the moment on the hard disk, rather than in RAM. The toolbox makes this easy to do, using [http://www.csbmb.princeton.edu/mvpa/docs/m2html/move_pattern_to_hd.html ''move_pattern_to_hd.m'']. See tutorial_hard.htm / Moving patterns to the HD for more information.

Finally, you can always just [#_Removing_objects remove the object].

See also:

''Exporting / ''[#_What_if_I ''What if I get an 'out of memory' error when writing BRIKs]'


===== Amount of required RAM =====

In order to do anything with a Matlab matrix, you really need enough RAM to store it twice. Removing or adding a column to it, or modifying it within a function, require a duplication of that matrix. We have endeavoured to ensure that the toolbox never requires you to have more RAM than needed to hold a single duplicate of your data. Some of the toolbox's internal accessor functions (such as the ''set/get_object'' functions, and the ''init_new_object'' functions) may look incredibly inefficient, since it seems as though they are making multiple copies of entire objects or even entire cell arrays of objects and passing them up and down functions. The advantage of this coding style is that all of the accessor logic for the 4 data types is in one place. Fortunately, even though Matlab passes by value and doesn't explicitly allow the use of pointers, it appears to implicitly use pointers in a clever way to ensure that it only actually makes a copy of an argument when passing it into a function if the contents of that argument get modified.

Having experimented with different ways of storing and passing objects between functions, we don't think the toolbox could manage its variables much more efficiently, short of shunning functions entirely. If you have any better ideas, or believe this explanation to be in error, we would be very interested to [#_Contact_details hear from you].

Finally, don't forget that you can [#_Moving_patterns_to store your patterns on the hard disk rather than in RAM] when they're not being used.


==== Moving patterns to the hard disk ====

The [http://www.csbmb.princeton.edu/mvpa/docs/m2html/move_pattern_to_hd.html ''move_pattern_to_hd.m''] and its companion, [http://www.csbmb.princeton.edu/mvpa/docs/m2html/load_pattern_from_hd.html ''load_pattern_from_hd.m''] are designed to free up RAM without throwing away information. They allow you to move the matrix contents of a particular object out to a file, while allowing you to access that data using the standard ''get/set_object'' scripts entirely transparently. This is one of the side-advantages of using the accessor scripts as a layer of abstraction between the toolbox's data structures and you, the user.

If you're having trouble with memory, we recommend that you choose a couple of large pattern objects that you're not using and use ''move_pattern_to_hd.m'' on them to free up RAM. Of course, access times for data stored on the hard disk are much, much slower than for data stored in RAM. For this reason, the data can be moved back into RAM (removing any traces from the hard disk) with ''load_pattern_from_hd.m''.

In the future, we hope to take advantage of Matlab's memory-mapping functionality or investigate other file formats that will allow more efficient random-access than the .mat format, which requires the entire matrix to be loaded in before it can be accessed. Any advice or support [#_Contact_details would be appreciated].

It is worth noting that this functionality only exists for ''patterns'', since they take up so much more space than any of the other data types.

See [tutorial_hard.htm ''tutorial_hard'']'' / Moving patterns to the HD for a walkthrough of how to do this''.


==== Removing objects ====

There are two ways in which you can remove objects:

 1. [http://www.csbmb.princeton.edu/mvpa/docs/m2html/remove_mat.html ''Remove_mat.m''] will set the object's ''mat'' to empty. Since this is where each object's main matrix is stored, this is probably taking up most of the room occupied by the object. This is usually sufficient, and has the advantage that it leaves the surrounding ''name'' and ''header ''information intact, in case other objects want to reference it, and doesn't renumber the cells.
 1. If you really want to, you can remove the entire cell for the object, by passing in 'erase' as the optional argument to [http://www.csbmb.princeton.edu/mvpa/docs/m2html/remove_object.html ''remove_object.m''].

Since ''patterns'' tend to be the biggest memory hogs, you can also just transparently [#_Moving_patterns_to shift the ''pattern'' to the hard disk], which frees up your RAM without thowing away data that might be useful later on. See ''Advanced / ''[#_Managing_memory ''Managing memory''] for more solutions.


==== Accessing the subj structure directly ====

We realise that using the accessor functions is slightly more cumbersome than accessing the contents of the ''subj'' structure directly. However, bypassing them can cause many subtle problems and is not recommended.

Having said that, nothing can go wrong if you're just using the standard syntax to view an object, e.g.

>> subj.regressors{5}

It's really only when you change the values in an object that there's a real risk of things getting badly confused amidst the assumptions and interactions between functions. Hopefully, these should all be preserved intact by the provided accessor functions.

We will provide one example of how the accessor scripts can help. If you remove voxels from a ''pattern ''by just deleting them from the matrix, this could disrupt all the fragile indexing that is used to determine which voxel is which. In order to avoid this, ''set_mat.m'' makes it very difficult to change the dimensions of the ''pattern'' matrix, unless you are using the [#_Figuring_out_which authorised masking functionality that preserves the indexing]. Such bugs can be particularly insidious because they may or may not cause errors. In the worst case, your analysis could run without incident, but actually scramble your voxels.

These [#_Handy_shortcuts handy shortcuts] may make your life a little easier.


==== Figuring out which voxel is which, and where ====

''Or "Dude, Where's My Voxel'" ' A Guide to Relative Indices in MVPA by Chris Moore''

See also: [#_Masks Howto's / Data structures / Masks].

In a perfect world, patterns stored in MVPA subject structures would be stored in their native space, in full dimensionality. But due to memory contraints and convenience, it is typically prudent to input and store a subset of the data, such as the voxels specified by a whole-brain mask. For this reason, loaded 3D patterns are stored in the ''subj'' structure as two dimensional matrices.

It is important to note that the 3D information (xyz coordinates of voxels in a pattern) is ''not'' stored in the pattern once it is loaded. Rather, all patterns are associated with the mask that was used in their creation. For instance, when calling ''[http://www.csbmb.princeton.edu/mvpa/docs/m2html/load_afni_pattern.html load_afni_pattern.m]'', a required argument is the name of a map in the ''subj'' structure used to mask the BRIK. The resulting pattern contains a field, ''masked_by'', which points to the mask that contains the 3D information.

The mask should be a boolean mask with the same dimensions as the data in its native space. The order of data in the pattern corresponds to the order of the voxels, such that one could recreate the dataset in its native space with the code:

{{{
>> data = get_mat(subj,'pattern','dataset'); 

>> masked_by = get_objfield(subj,'dataset','masked_by');

>> mask = get_mat(subj,'mask',masked_by);

>> full_data = zeros(size(mask));

>> full_data(find(mask)) = data; 
}}}

Note that this ''only'' works for datasets and masks when the mask was used to create the dataset, thus ensuring proper relative indices, and the mask has not been altered in any way. In general, it is a good practice to never modify masks. If you wish to modify a mask, consider creating a duplicate, and modifying the duplicate. If you do modify a mask that was used to create a dataset, you will break the relative indices, and will not be able to recover the XYZ coordinates of each voxel in that pattern. In the worst case, you may not realise that this is the case, and be erroneously indexing your voxels.

Relative indices such as those above (e.g., find(mask)) only work under specific conditions, and can cause problems if not used properly. For instance, if we have a wholebrain data set, and mask it with an anatomical mask, we can create a 2nd pattern. If we then take this pattern and apply a functional mask, we create a 3rd pattern. The relative indices from the 3rd pattern to the 1st and 2nd are different. To avoid this confusion, they are never stored within the ''subj'' structure. Relative indices should always be created on the fly between two objects, and never stored.

Relative indices between any two objects can be calculated by finding the intersection of two masks. Thus, when finding the relative indices for a pattern and a mask, the pattern must have a ''masked_by'' mask intact. The code for calculating relative indices is as follows:

{{{
>> data = get_mat(subj,'pattern','dataset'); 

>> masked_by = get_objfield(subj,'pattern','dataset','masked_by');

>> mask = get_mat(subj,'mask','masked_by');

>> new_mask = get_mat(subj,'mask','anat_mask');

>> [int ia ib] = intersect(find(mask), find(new_mask));
 }}}

In this example, IA is the index of MASK in NEW_MASK. IB is the inverse, the index of NEW_MASK in MASK. Thus, one could take the relative index, IB, and extract the voxels in DATA that are present in the mask NEW_MASK:

{{{ 
>> new_data = data(IB,:);
 }}}

This is precisely what create_thresh_mask() does. We have included this guide to help users understand how relative indices were meant to work, and to facilitate integrating new functions, but urge the user to use the standard MVPA functions whenever possible. Happy coding.


==== Handy shortcuts ====

We've tried to add some minor labour-saving devices to make using the toolbox easier.


===== Getting the latest object names =====

Much of the time, you'll want to look at or use the most recently-created pattern, or run or regressors or mask. Sometimes, it can be a pain to remember or type long object names, and sometimes when writing functions, you may not know in advance what the latest names will be.

To make this easier, the toolbox keeps track of the latest object of each type that was initialized as subj.p, .r, .s and .m. This way, you could type:

{{{
>> get_object(subj,'pattern',subj.p) 

as a bit of a shorthand. 
}}}

Note: this is one of those handy labour-saving devices that might prove a terrible idea if used carelessly. For instance, be careful after creating a group, since it's unlikely that you'll want to specifically access the last item in that group.

Note: We deliberately decided not to have the remove functions alter the subj.x shortcuts when removing the highest-numbered mat or object of its type. This way, if you forget to update them yourself, you'll get an error. If we were to automatically set them to the next highest-numbered item, you might not notice and end up with a subtil and devilish bug.

If you have ideas for other ways to speed up frequently-performed tasks or minimise how much typing users have to do, we'd be happy to [#_Contact_details hear from you].


===== Getting the mat immediately from a duplicated object =====

Often, you'll find yourself doing the following:

1. duplicating an object

2. getting the ''mat'' from the new object

3. modifying it

4. setting it back into the object.

To reduce this down by one step, it's worth noting that the [http://www.csbmb.princeton.edu/mvpa/docs/m2html/duplicate_object.html ''duplicate_object.m''] function will return the duplicated object's ''mat'' as its second argument, allowing you to combine the first two steps in one line:

{{{ 
>> [subj duplicated_mat] = duplicate_obj(subj,objtype,old_objname,new_objname);
 }}}


==== Creating custom functions ====

It is very easy to override the toolbox's default functions with your own custom-created function that the main toolbox control functions can call when appropriate. For instance, if you don't want to use the default backpropagation classifier, or the default ANOVA statmap generator, you can drop your own functions in instead, and the toolbox's no-peeking cross-validation control functions will use your custom functions instead at the appropriate times.

At the moment, there are various places where you can substitute your own functions ' click on the links in the [#_Places_that_can list] below to skip to the section where the specific details for each are described.

If you do create a custom function and you think others might benefit from it, we'd really like to [#_Contact_details hear from you] so that we can incorporate it into future releases of the toolbox.


===== Places that can call custom function =====

 * [#_statmap statmap generation] ' ''feature_selection.m'' and ''peek_feature_selection.m'' can take optional ''statmap_funct'' and ''statmap_arg'' arguments
 * [#_Creating_your_own classifier training] ' ''cross_validation.m'' can take an optional ''train_funct'' function name string
 * [#_Creating_your_own classifier test] ' ''cross_validation.m'' can take an optional ''test_funct'' function name string
 * [#_Creating_your_own classifier performance metrics] ' ''cross_validation.m'' can take an optional ''perfmet_functs'' cell array of performance metric function name strings


===== Requirements for custom functions that modify the ''subj'' structure =====

Any custom functions that modify the ''subj'' structure (usually by creating a new object) should fulfil the following requirements, if they're going to be well-behaved toolbox citizens:

Â· take in a ''subj'' structure as their first argument

Â· return the modified ''subj'' structure as their first output argument

Â· if the custom function creates a new ''subj'' structure object, you should add a line to the help comments that says 'Adds the following objects:' and a list of the objects/groups that get created

Â· if the custom function creates a new ''subj'' structure object, you should display something like:

sprintf('Created %s called %s',objtype,objname)

 * should call the [http://www.csbmb.princeton.edu/mvpa/docs/m2html/add_created.html ''add_created.m'']'' ''with fields for the name of the function and any arguments it takes
 * should call [http://www.csbmb.princeton.edu/mvpa/docs/m2html/add_history.html ''add_history.m'']'' ''to add a line describing themselves to their own freetext history narrative
 * try and do some error-checking on the inputs, if there are any assumptions that the function makes, to help future users avoid making hard-to-debug booboos


==== Optional arguments' ====

All of the functions in the toolbox use the same conventions for optional arguments, since they all rely on ''[http://www.csbmb.princeton.edu/mvpa/docs/m2html/propval.html propval.m]''. This is a standalone function that makes it very easy to specify what optional arguments a function should accept, in any order, what default arguments it should use if they're not supplied, and with lots of error-checking and warning.

Optional arguments must be supplied in property/value pairs, e.g.

{{{
>> summarize(subj,'display_groups',false) 

Here, the property being specified is 'display_groups' and the value being specified is ''false''. Multiple optional arguments can be specified at once, and in any order, e.g.

>> summarize(subj,'display_groups',false,'objtype','selector')

or:

>> summarize(subj,'objtype','selector','display_groups',false)
 }}}

In this case, the property/value pairings are as follows:

||<style="border: 0.5pt solid windowtext; padding: 0cm 5.4pt;"> ''Property'' ||<style="border-style: solid solid solid none; border-color: windowtext windowtext windowtext -moz-use-text-color; border-width: 0.5pt 0.5pt 0.5pt medium; padding: 0cm 5.4pt;"> ''Value'' ||
||<style="border-style: none solid solid; border-color: -moz-use-text-color windowtext windowtext; border-width: medium 0.5pt 0.5pt; padding: 0cm 5.4pt;"> display_groups ||<style="border-style: none solid solid none; border-color: -moz-use-text-color windowtext windowtext -moz-use-text-color; border-width: medium 0.5pt 0.5pt medium; padding: 0cm 5.4pt;"> false ||
||<style="border-style: none solid solid; border-color: -moz-use-text-color windowtext windowtext; border-width: medium 0.5pt 0.5pt; padding: 0cm 5.4pt;"> objtype ||<style="border-style: none solid solid none; border-color: -moz-use-text-color windowtext windowtext -moz-use-text-color; border-width: medium 0.5pt 0.5pt medium; padding: 0cm 5.4pt;"> 'selector' ||


Properties are always strings, and always come first in the pair. Values can be strings, or any other type, and always come second.

By the way, for more information on the [http://www.csbmb.princeton.edu/mvpa/docs/m2html/summarize.html summarize.m] function, see also: [#_Viewing_the_subj Viewing the subj structure] and [:MVPA manual:How can I slim down the output from summarize.m].

In the help for a function, the triple-dot at the end of the function declaration denotes that it takes optional arguments, e.g.

{{{
>> help summarize 

[] = summarize(subj,...)

Below, the help says:

DISPLAY_GROUPS (optional, default = blah) ' blah blah

OBJTYPE (optional, default = blah) ' blah blah
 }}}

In this way, all the allowed optional argument properties will be listed, along with their default values (if they are left unspecified), and what terrible things will be wrought by each.

This is a powerful and flexible mechanism, since it allows us to keep basic function declarations simple if you want to use the defaults, but doesn't restrict the user if they do want to specify niceties.

There is one further way in which optional arguments can be supplied. If calling a function with lots of optional arguments, it can be a pain to specify them all each time. In this case, you can bundle them all together in a structure, and just feed that in. Propval.m will understand and deconstruct the structure in exactly the same way as before, e.g.

{{{
>> summ_args.display_groups = false; 

>> summ_args.objtype = 'selector';

>> summarize(subj,summ_args)
 }}}

See the help for [http://www.csbmb.princeton.edu/mvpa/docs/m2html/propval.html propval.m] for more information.

Note: there is one exception - [http://www.csbmb.princeton.edu/mvpa/docs/m2html/summarize.html summarize.m] has a special single optional boolean argument that determines whether or not to display all the members of a group. This is for the user's convenience, since ''summarize.m'' gets called so much, but may be deprecated in future versions.


[[Include(GoodPractices)]]

[[Include(Troubleshooting)]]

[[Include(Howtos)]]